# Config parameters for CausalLMRouter training:


data_path:
  query_data_train: 'data/example_data/query_data/default_query_train.jsonl'
  query_data_test: 'data/example_data/query_data/default_query_test.jsonl'
  query_embedding_data: 'data/example_data/routing_data/query_embeddings_longformer.pt'
  routing_data_train: 'data/example_data/routing_data/default_routing_train_data.jsonl'
  routing_data_test: 'data/example_data/routing_data/default_routing_test_data.jsonl'
  llm_data: 'data/example_data/llm_candidates/default_llm.json'
  llm_embedding_data: 'data/example_data/llm_candidates/default_llm_embeddings.json'

model_path:
  ini_model_path: ''
  save_model_path: 'saved_models/causallm_router'
  load_model_path: 'saved_models/causallm_router/merged'

metric:
  weights:
    performance: 1
    cost: 0
    llm_judge: 0

hparam:
  # Base model
  base_model: "meta-llama/Llama-2-7b-hf"    # Base model for finetuning

  # LoRA config
  use_lora: true                             # Whether to use LoRA
  lora_r: 16                                 # LoRA rank
  lora_alpha: 32                             # LoRA alpha
  lora_dropout: 0.1                          # LoRA dropout
  lora_target_modules:                       # Target modules for LoRA
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  merge_lora: true                           # Whether to merge LoRA after training

  # Training config
  num_epochs: 3                              # Number of training epochs
  batch_size: 4                              # Per-device batch size
  gradient_accumulation_steps: 4             # Gradient accumulation steps
  learning_rate: 0.00002                     # Learning rate
  weight_decay: 0.01                         # Weight decay
  warmup_ratio: 0.1                          # Warmup ratio
  max_length: 512                            # Max sequence length
  fp16: true                                 # Use mixed precision

  # Logging
  logging_steps: 10                          # Log every N steps
  save_steps: 100                            # Save checkpoint every N steps
  report_to: "none"                          # Reporting backend (none, wandb, tensorboard)

  # Inference config (for vLLM)
  tensor_parallel_size: 1                    # Number of GPUs for tensor parallelism
  gpu_memory_utilization: 0.9                # GPU memory utilization
  max_new_tokens: 32                         # Max tokens to generate
  temperature: 0.1                           # Sampling temperature
  top_p: 0.95                                # Top-p sampling



