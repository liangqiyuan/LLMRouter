{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CausalLMRouter - Training\n",
    "\n",
    "This notebook demonstrates how to train the **CausalLMRouter** (Causal Language Model Router).\n",
    "\n",
    "## Overview\n",
    "\n",
    "CausalLMRouter finetunes a causal language model (e.g., Llama-2-7B) to predict the best LLM for routing.\n",
    "It uses LoRA (Low-Rank Adaptation) for efficient finetuning.\n",
    "\n",
    "**Key Features**:\n",
    "- Uses powerful LLM backbone (Llama-2)\n",
    "- Efficient LoRA finetuning\n",
    "- Can understand complex query semantics\n",
    "- Supports vLLM for fast inference\n",
    "\n",
    "**Requirements**:\n",
    "- GPU with at least 16GB VRAM recommended\n",
    "- HuggingFace access to Llama-2 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (for Colab)\n",
    "# !pip install llmrouter transformers torch peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llmrouter.models.causallm_router import CausalLMRouter, CausalLMTrainer\n",
    "from llmrouter.utils import setup_environment\n",
    "\n",
    "setup_environment()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace login (required for Llama-2 access)\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"your_hf_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "CausalLMRouter uses the following configuration parameters:\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|--------|\n",
    "| `base_model` | Base LLM for finetuning | \"meta-llama/Llama-2-7b-hf\" |\n",
    "| `use_lora` | Enable LoRA finetuning | true |\n",
    "| `lora_r` | LoRA rank | 16 |\n",
    "| `lora_alpha` | LoRA alpha | 32 |\n",
    "| `lora_dropout` | LoRA dropout | 0.1 |\n",
    "| `num_epochs` | Training epochs | 3 |\n",
    "| `batch_size` | Batch size | 4 |\n",
    "| `learning_rate` | Learning rate | 2e-5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "CONFIG_PATH = \"configs/model_config_train/causallm_router.yaml\"\n",
    "\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Current Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router = CausalLMRouter(yaml_path=CONFIG_PATH)\n",
    "\n",
    "print(\"Router initialized successfully!\")\n",
    "print(f\"Number of training samples: {len(router.routing_data_train)}\")\n",
    "print(f\"Number of LLM candidates: {len(router.llm_data)}\")\n",
    "print(f\"LLM candidates: {list(router.llm_data.keys())}\")\n",
    "print(f\"Base model: {config['hparam']['base_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the training data format\n",
    "print(\"Training Data Format:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nThe model is trained to predict the best LLM given a query.\")\n",
    "print(\"\\nInput format:\")\n",
    "print(\"  Query: {user query}\")\n",
    "print(\"  Best model: \")\n",
    "print(\"\\nTarget format:\")\n",
    "print(\"  {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CausalLMTrainer(router=router, device=device)\n",
    "\n",
    "print(\"Trainer initialized!\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Save path: {trainer.save_model_path}\")\n",
    "print(f\"LoRA enabled: {config['hparam'].get('use_lora', True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show trainable parameters\n",
    "if hasattr(trainer, 'model'):\n",
    "    total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Note: CausalLM training requires significant GPU memory.\")\n",
    "print(\"Consider reducing batch_size if you encounter OOM errors.\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check saved model\n",
    "import os\n",
    "\n",
    "save_path = trainer.save_model_path\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"Model saved at: {save_path}\")\n",
    "    \n",
    "    # List saved files\n",
    "    if os.path.isdir(save_path):\n",
    "        files = os.listdir(save_path)\n",
    "        print(f\"\\nSaved files:\")\n",
    "        for f in files:\n",
    "            size = os.path.getsize(os.path.join(save_path, f)) / 1e6\n",
    "            print(f\"  {f}: {size:.2f} MB\")\n",
    "else:\n",
    "    print(f\"Model not found at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "test_query = {\"query\": \"What is the capital of France?\"}\n",
    "result = router.route_single(test_query)\n",
    "\n",
    "print(f\"Test query: {test_query['query']}\")\n",
    "print(f\"Routed to: {result['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Loaded Configuration**: Set up CausalLMRouter with YAML configuration\n",
    "2. **Initialized Router**: Created router with Llama-2 backbone\n",
    "3. **Applied LoRA**: Efficient finetuning with low-rank adaptation\n",
    "4. **Trained Model**: Finetuned to predict best LLM for queries\n",
    "5. **Saved Model**: LoRA weights and merged model saved\n",
    "\n",
    "**Key Takeaways**:\n",
    "- CausalLMRouter uses powerful LLM understanding\n",
    "- LoRA enables efficient finetuning (only ~0.1% params trainable)\n",
    "- Requires GPU with sufficient memory\n",
    "\n",
    "**Next Steps**:\n",
    "- Use `02_causallm_router_inference.ipynb` for inference\n",
    "- Consider using vLLM for faster inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
