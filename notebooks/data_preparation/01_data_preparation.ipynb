{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMRouter - Data Preparation\n",
    "\n",
    "This notebook covers the complete data preparation pipeline for LLMRouter:\n",
    "1. **Dataset Download**: Download benchmark datasets from HuggingFace\n",
    "2. **Query Data Generation**: Generate query data JSONL files\n",
    "3. **LLM Embeddings Generation**: Generate LLM feature embeddings\n",
    "4. **API Calling & Evaluation**: Call LLM APIs and evaluate responses\n",
    "5. **Final Routing Data**: Generate unified embeddings and routing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (for Colab)\n",
    "# !pip install llmrouter datasets transformers torch pandas numpy tqdm litellm peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Set project root\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LLMRouter utilities\n",
    "from llmrouter.utils import setup_environment\n",
    "from llmrouter.data.data_loader import DataLoader\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for data generation\n",
    "CONFIG = {\n",
    "    # Number of samples per task\n",
    "    \"sample_size\": 100,  # Set to smaller number for testing, increase for full training\n",
    "    \n",
    "    # Train/test split ratio\n",
    "    \"train_ratio\": 0.8,\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    # Output paths (relative to project root)\n",
    "    \"output_paths\": {\n",
    "        \"query_data_train\": \"data/example_data/query_data/default_query_train.jsonl\",\n",
    "        \"query_data_test\": \"data/example_data/query_data/default_query_test.jsonl\",\n",
    "        \"query_embedding_data\": \"data/example_data/routing_data/query_embeddings_longformer.pt\",\n",
    "        \"routing_data_train\": \"data/example_data/routing_data/default_routing_train_data.jsonl\",\n",
    "        \"routing_data_test\": \"data/example_data/routing_data/default_routing_test_data.jsonl\",\n",
    "        \"llm_data\": \"data/example_data/llm_candidates/default_llm.json\",\n",
    "        \"llm_embedding_data\": \"data/example_data/llm_candidates/default_llm_embeddings.json\"\n",
    "    },\n",
    "    \n",
    "    # API settings (for LLM calling)\n",
    "    \"max_workers\": 10,  # Number of parallel API calls\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Download\n",
    "\n",
    "LLMRouter uses 11 benchmark datasets covering different task categories:\n",
    "- **Math**: GSM8K, MATH\n",
    "- **Code**: MBPP, HumanEval\n",
    "- **World Knowledge**: Natural QA, Trivia QA\n",
    "- **Popular Benchmarks**: MMLU, GPQA\n",
    "- **Commonsense Reasoning**: CommonsenseQA, OpenbookQA, ARC-Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Set cache directory (optional)\n",
    "CACHE_DIR = str(PROJECT_ROOT / \"data\" / \"cache\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def download_datasets(sample_size=100, random_seed=42):\n",
    "    \"\"\"Download and sample from benchmark datasets.\"\"\"\n",
    "    random.seed(random_seed)\n",
    "    samples = {}\n",
    "    \n",
    "    # 1. Natural QA\n",
    "    print(\"Downloading Natural QA...\")\n",
    "    try:\n",
    "        natural_qa = load_dataset('RUC-NLPIR/FlashRAG_datasets', 'nq', cache_dir=CACHE_DIR)\n",
    "        split_name = 'train' if 'train' in natural_qa else list(natural_qa.keys())[0]\n",
    "        indices = random.sample(range(len(natural_qa[split_name])), min(sample_size, len(natural_qa[split_name])))\n",
    "        samples['natural_qa'] = [natural_qa[split_name][i] for i in indices]\n",
    "        print(f\"  Extracted {len(samples['natural_qa'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        samples['natural_qa'] = []\n",
    "    \n",
    "    # 2. Trivia QA\n",
    "    print(\"Downloading Trivia QA...\")\n",
    "    try:\n",
    "        trivia_qa = load_dataset(\"trivia_qa\", \"rc.nocontext\", cache_dir=CACHE_DIR)\n",
    "        split_name = 'train' if 'train' in trivia_qa else list(trivia_qa.keys())[0]\n",
    "        indices = random.sample(range(len(trivia_qa[split_name])), min(sample_size, len(trivia_qa[split_name])))\n",
    "        samples['trivia_qa'] = [trivia_qa[split_name][i] for i in indices]\n",
    "        print(f\"  Extracted {len(samples['trivia_qa'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        samples['trivia_qa'] = []\n",
    "    \n",
    "    # 3. MMLU\n",
    "    print(\"Downloading MMLU...\")\n",
    "    try:\n",
    "        mmlu = load_dataset(\"cais/mmlu\", \"all\", cache_dir=CACHE_DIR)\n",
    "        split_name = 'auxiliary_train' if 'auxiliary_train' in mmlu else list(mmlu.keys())[0]\n",
    "        indices = random.sample(range(len(mmlu[split_name])), min(sample_size, len(mmlu[split_name])))\n",
    "        samples['mmlu'] = [mmlu[split_name][i] for i in indices]\n",
    "        print(f\"  Extracted {len(samples['mmlu'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        samples['mmlu'] = []\n",
    "    \n",
    "    # 4. GPQA\n",
    "    print(\"Downloading GPQA...\")\n",
    "    try:\n",
    "        gpqa = load_dataset(\"Idavidrein/gpqa\", \"gpqa_main\", cache_dir=CACHE_DIR)\n",
    "        split_name = 'train' if 'train' in gpqa else list(gpqa.keys())[0]\n",
    "        indices = random.sample(range(len(gpqa[split_name])), min(sample_size, len(gpqa[split_name])))\n",
    "        samples['gpqa'] = [gpqa[split_name][i] for i in indices]\n",
    "        print(f\"  Extracted {len(samples['gpqa'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        samples['gpqa'] = []\n",
    "    \n",
    "    # 5. GSM8K\n",
    "    print(\"Downloading GSM8K...\")\n",
    "    try:\n",
    "        gsm8k = load_dataset('gsm8k', 'main', cache_dir=CACHE_DIR)\n",
    "        split_name = 'train' if 'train' in gsm8k else list(gsm8k.keys())[0]\n",
    "        indices = random.sample(range(len(gsm8k[split_name])), min(sample_size, len(gsm8k[split_name])))\n",
    "        samples['gsm8k'] = [gsm8k[split_name][i] for i in indices]\n",
    "        print(f\"  Extracted {len(samples['gsm8k'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        samples['gsm8k'] = []\n",
    "    \n",
    "    # 6. CommonsenseQA\n",
    "    print(\"Downloading CommonsenseQA...\")\n",
    "    try:\n",
    "        commonsense_qa = load_dataset('commonsense_qa', cache_dir=CACHE_DIR)\n",
    "        split_name = 'train' if 'train' in commonsense_qa else list(commonsense_qa.keys())[0]\n",
    "        indices = random.sample(range(len(commonsense_qa[split_name])), min(sample_size, len(commonsense_qa[split_name])))\n",
    "        samples['commonsense_qa'] = [commonsense_qa[split_name][i] for i in indices]\n",
    "        print(f\"  Extracted {len(samples['commonsense_qa'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        samples['commonsense_qa'] = []\n",
    "    \n",
    "    # 7. ARC-Challenge\n",
    "    print(\"Downloading ARC-Challenge...\")\n",
    "    try:\n",
    "        arc = load_dataset('allenai/ai2_arc', 'ARC-Challenge', cache_dir=CACHE_DIR)\n",
    "        split_name = 'train' if 'train' in arc else list(arc.keys())[0]\n",
    "        indices = random.sample(range(len(arc[split_name])), min(sample_size, len(arc[split_name])))\n",
    "        samples['arc_challenge'] = [arc[split_name][i] for i in indices]\n",
    "        print(f\"  Extracted {len(samples['arc_challenge'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        samples['arc_challenge'] = []\n",
    "    \n",
    "    # 8. OpenbookQA\n",
    "    print(\"Downloading OpenbookQA...\")\n",
    "    try:\n",
    "        openbook = load_dataset('allenai/openbookqa', 'main', cache_dir=CACHE_DIR)\n",
    "        split_name = 'train' if 'train' in openbook else list(openbook.keys())[0]\n",
    "        indices = random.sample(range(len(openbook[split_name])), min(sample_size, len(openbook[split_name])))\n",
    "        samples['openbook_qa'] = [openbook[split_name][i] for i in indices]\n",
    "        print(f\"  Extracted {len(samples['openbook_qa'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        samples['openbook_qa'] = []\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Download datasets\n",
    "raw_samples = download_datasets(sample_size=CONFIG['sample_size'], random_seed=CONFIG['random_seed'])\n",
    "print(f\"\\nTotal tasks downloaded: {len([k for k, v in raw_samples.items() if v])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Data Generation\n",
    "\n",
    "Convert raw samples into standardized query format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_samples_to_query_data(samples):\n",
    "    \"\"\"Convert raw samples to standardized query data format.\"\"\"\n",
    "    data_all = []\n",
    "    \n",
    "    # Process Natural QA\n",
    "    for sample in samples.get('natural_qa', []):\n",
    "        data_all.append({\n",
    "            'task_name': 'natural_qa',\n",
    "            'query': sample['question'],\n",
    "            'ground_truth': sample['golden_answers'][0] if sample.get('golden_answers') else sample.get('answer', ''),\n",
    "            'metric': 'f1_score',\n",
    "            'choices': None,\n",
    "            'task_id': None\n",
    "        })\n",
    "    \n",
    "    # Process Trivia QA\n",
    "    for sample in samples.get('trivia_qa', []):\n",
    "        data_all.append({\n",
    "            'task_name': 'trivia_qa',\n",
    "            'query': sample['question'],\n",
    "            'ground_truth': sample['answer']['normalized_aliases'][0] if sample.get('answer') else '',\n",
    "            'metric': 'f1_score',\n",
    "            'choices': None,\n",
    "            'task_id': None\n",
    "        })\n",
    "    \n",
    "    # Process MMLU\n",
    "    for sample in samples.get('mmlu', []):\n",
    "        data_all.append({\n",
    "            'task_name': 'mmlu',\n",
    "            'query': sample['question'],\n",
    "            'ground_truth': chr(65 + sample['answer']),  # Convert 0,1,2,3 to A,B,C,D\n",
    "            'metric': 'em_mc',\n",
    "            'choices': {'text': sample['choices'], 'labels': ['A', 'B', 'C', 'D']},\n",
    "            'task_id': None\n",
    "        })\n",
    "    \n",
    "    # Process GPQA\n",
    "    for sample in samples.get('gpqa', []):\n",
    "        options = [\n",
    "            sample['Correct Answer'],\n",
    "            sample['Incorrect Answer 1'],\n",
    "            sample['Incorrect Answer 2'],\n",
    "            sample['Incorrect Answer 3']\n",
    "        ]\n",
    "        # Shuffle options\n",
    "        mapping = list(range(4))\n",
    "        random.shuffle(mapping)\n",
    "        shuffled_options = [options[mapping.index(i)] for i in range(4)]\n",
    "        correct_idx = mapping.index(0)\n",
    "        \n",
    "        data_all.append({\n",
    "            'task_name': 'gpqa',\n",
    "            'query': sample['Question'],\n",
    "            'ground_truth': chr(65 + correct_idx),\n",
    "            'metric': 'em_mc',\n",
    "            'choices': {'text': shuffled_options, 'labels': ['A', 'B', 'C', 'D']},\n",
    "            'task_id': None\n",
    "        })\n",
    "    \n",
    "    # Process GSM8K\n",
    "    for sample in samples.get('gsm8k', []):\n",
    "        data_all.append({\n",
    "            'task_name': 'gsm8k',\n",
    "            'query': sample['question'],\n",
    "            'ground_truth': sample['answer'],\n",
    "            'metric': 'GSM8K',\n",
    "            'choices': None,\n",
    "            'task_id': None\n",
    "        })\n",
    "    \n",
    "    # Process CommonsenseQA\n",
    "    for sample in samples.get('commonsense_qa', []):\n",
    "        data_all.append({\n",
    "            'task_name': 'commonsense_qa',\n",
    "            'query': sample['question'],\n",
    "            'ground_truth': sample['answerKey'],\n",
    "            'metric': 'em_mc',\n",
    "            'choices': sample['choices'],\n",
    "            'task_id': None\n",
    "        })\n",
    "    \n",
    "    # Process ARC-Challenge\n",
    "    for sample in samples.get('arc_challenge', []):\n",
    "        data_all.append({\n",
    "            'task_name': 'arc_challenge',\n",
    "            'query': sample['question'],\n",
    "            'ground_truth': sample['answerKey'],\n",
    "            'metric': 'em_mc',\n",
    "            'choices': sample['choices'],\n",
    "            'task_id': None\n",
    "        })\n",
    "    \n",
    "    # Process OpenbookQA\n",
    "    for sample in samples.get('openbook_qa', []):\n",
    "        data_all.append({\n",
    "            'task_name': 'openbook_qa',\n",
    "            'query': sample['question_stem'],\n",
    "            'ground_truth': sample['answerKey'],\n",
    "            'metric': 'em_mc',\n",
    "            'choices': sample['choices'],\n",
    "            'task_id': None\n",
    "        })\n",
    "    \n",
    "    return data_all\n",
    "\n",
    "# Process samples\n",
    "query_data = process_samples_to_query_data(raw_samples)\n",
    "print(f\"Total processed samples: {len(query_data)}\")\n",
    "\n",
    "# Show sample counts by task\n",
    "from collections import Counter\n",
    "task_counts = Counter(item['task_name'] for item in query_data)\n",
    "for task, count in sorted(task_counts.items()):\n",
    "    print(f\"  {task}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "random.seed(CONFIG['random_seed'])\n",
    "random.shuffle(query_data)\n",
    "\n",
    "train_size = int(len(query_data) * CONFIG['train_ratio'])\n",
    "train_data = query_data[:train_size]\n",
    "test_data = query_data[train_size:]\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save query data to JSONL files\n",
    "def save_query_data_jsonl(data_list, output_path):\n",
    "    \"\"\"Save query data to JSONL file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data_list:\n",
    "            record = {\n",
    "                'task_name': item['task_name'],\n",
    "                'query': item['query'],\n",
    "                'ground_truth': item['ground_truth'],\n",
    "                'metric': item['metric'],\n",
    "                'choices': json.dumps(item['choices']) if item['choices'] is not None else None,\n",
    "                'task_id': item['task_id']\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"Saved {len(data_list)} records to {output_path}\")\n",
    "\n",
    "# Save files\n",
    "train_output_path = str(PROJECT_ROOT / CONFIG['output_paths']['query_data_train'])\n",
    "test_output_path = str(PROJECT_ROOT / CONFIG['output_paths']['query_data_test'])\n",
    "\n",
    "save_query_data_jsonl(train_data, train_output_path)\n",
    "save_query_data_jsonl(test_data, test_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LLM Candidates Configuration\n",
    "\n",
    "Define the LLM candidates for routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example LLM candidates configuration\n",
    "# You can modify this based on your available LLMs\n",
    "\n",
    "LLM_CANDIDATES = {\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"size\": \"small\",\n",
    "        \"feature\": \"Fast and cost-effective model for simple tasks\",\n",
    "        \"input_price\": 0.15,\n",
    "        \"output_price\": 0.60,\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"service\": \"OpenAI\",\n",
    "        \"api_endpoint\": \"https://api.openai.com/v1\"\n",
    "    },\n",
    "    \"gpt-4o\": {\n",
    "        \"size\": \"large\",\n",
    "        \"feature\": \"Most capable GPT-4 model for complex tasks\",\n",
    "        \"input_price\": 2.50,\n",
    "        \"output_price\": 10.00,\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"service\": \"OpenAI\",\n",
    "        \"api_endpoint\": \"https://api.openai.com/v1\"\n",
    "    },\n",
    "    \"claude-3-haiku\": {\n",
    "        \"size\": \"small\",\n",
    "        \"feature\": \"Fast and efficient Claude model\",\n",
    "        \"input_price\": 0.25,\n",
    "        \"output_price\": 1.25,\n",
    "        \"model\": \"claude-3-haiku-20240307\",\n",
    "        \"service\": \"Anthropic\",\n",
    "        \"api_endpoint\": \"https://api.anthropic.com/v1\"\n",
    "    },\n",
    "    \"claude-3-5-sonnet\": {\n",
    "        \"size\": \"large\",\n",
    "        \"feature\": \"Balanced Claude model for most tasks\",\n",
    "        \"input_price\": 3.00,\n",
    "        \"output_price\": 15.00,\n",
    "        \"model\": \"claude-3-5-sonnet-20241022\",\n",
    "        \"service\": \"Anthropic\",\n",
    "        \"api_endpoint\": \"https://api.anthropic.com/v1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save LLM candidates\n",
    "llm_data_path = str(PROJECT_ROOT / CONFIG['output_paths']['llm_data'])\n",
    "os.makedirs(os.path.dirname(llm_data_path), exist_ok=True)\n",
    "\n",
    "with open(llm_data_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(LLM_CANDIDATES, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(LLM_CANDIDATES)} LLM candidates to {llm_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Query Embeddings\n",
    "\n",
    "Generate Longformer embeddings for all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import embedding function\n",
    "from llmrouter.utils import get_longformer_embedding\n",
    "\n",
    "def generate_query_embeddings(query_data_list):\n",
    "    \"\"\"Generate Longformer embeddings for all queries.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for item in tqdm(query_data_list, desc=\"Generating embeddings\"):\n",
    "        query = item['query']\n",
    "        embedding = get_longformer_embedding(query)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "# Combine train and test data for unified embeddings\n",
    "all_data = train_data + test_data\n",
    "\n",
    "print(f\"Generating embeddings for {len(all_data)} queries...\")\n",
    "all_embeddings = generate_query_embeddings(all_data)\n",
    "\n",
    "print(f\"Embeddings shape: {all_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "embedding_path = str(PROJECT_ROOT / CONFIG['output_paths']['query_embedding_data'])\n",
    "os.makedirs(os.path.dirname(embedding_path), exist_ok=True)\n",
    "\n",
    "torch.save(all_embeddings, embedding_path)\n",
    "print(f\"Saved embeddings to {embedding_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. API Calling and Evaluation (Optional)\n",
    "\n",
    "This step calls LLM APIs to generate responses and evaluates their performance.\n",
    "\n",
    "**Note**: This step requires API keys to be set in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API keys (replace with your actual keys)\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-openai-key'\n",
    "# os.environ['ANTHROPIC_API_KEY'] = 'your-anthropic-key'\n",
    "# os.environ['API_KEYS'] = json.dumps(['key1', 'key2'])  # For multiple keys\n",
    "\n",
    "# Check if API keys are set\n",
    "api_keys_available = bool(os.environ.get('API_KEYS') or os.environ.get('OPENAI_API_KEY'))\n",
    "print(f\"API keys available: {api_keys_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell if you don't have API keys\n",
    "# This will use the existing routing data from the repository\n",
    "\n",
    "if api_keys_available:\n",
    "    from llmrouter.utils import call_api, generate_task_query\n",
    "    import pandas as pd\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    \n",
    "    def call_llm_for_query(args):\n",
    "        \"\"\"Call a single LLM for a query.\"\"\"\n",
    "        query_item, model_name, llm_config = args\n",
    "        \n",
    "        try:\n",
    "            # Format query based on task\n",
    "            formatted_query = generate_task_query(query_item['task_name'], query_item)\n",
    "            \n",
    "            # Prepare API request\n",
    "            request = {\n",
    "                'api_endpoint': llm_config['api_endpoint'],\n",
    "                'query': formatted_query,\n",
    "                'model_name': model_name,\n",
    "                'api_name': llm_config['model']\n",
    "            }\n",
    "            \n",
    "            # Call API\n",
    "            result = call_api(request, max_tokens=512, temperature=0.7)\n",
    "            \n",
    "            return {\n",
    "                **query_item,\n",
    "                'model_name': model_name,\n",
    "                'response': result.get('response', ''),\n",
    "                'prompt_tokens': result.get('prompt_tokens', 0),\n",
    "                'completion_tokens': result.get('completion_tokens', 0),\n",
    "                'success': 'error' not in result\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                **query_item,\n",
    "                'model_name': model_name,\n",
    "                'response': f'ERROR: {str(e)}',\n",
    "                'prompt_tokens': 0,\n",
    "                'completion_tokens': 0,\n",
    "                'success': False\n",
    "            }\n",
    "    \n",
    "    # Create tasks for all query-model combinations\n",
    "    tasks = []\n",
    "    for item in train_data[:10]:  # Limit for demo\n",
    "        for model_name, config in LLM_CANDIDATES.items():\n",
    "            tasks.append((item, model_name, config))\n",
    "    \n",
    "    print(f\"Processing {len(tasks)} query-model combinations...\")\n",
    "    \n",
    "    # Execute in parallel\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:\n",
    "        futures = {executor.submit(call_llm_for_query, task): task for task in tasks}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    print(f\"Completed {len(results)} API calls\")\n",
    "else:\n",
    "    print(\"Skipping API calling - no API keys configured\")\n",
    "    print(\"You can use the existing routing data from: data/example_data/routing_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using Existing Data\n",
    "\n",
    "If you skipped the API calling step, you can use the existing example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing routing data (if available)\n",
    "from llmrouter.utils import load_jsonl\n",
    "\n",
    "routing_train_path = str(PROJECT_ROOT / CONFIG['output_paths']['routing_data_train'])\n",
    "routing_test_path = str(PROJECT_ROOT / CONFIG['output_paths']['routing_data_test'])\n",
    "\n",
    "if os.path.exists(routing_train_path):\n",
    "    routing_train = load_jsonl(routing_train_path)\n",
    "    print(f\"Loaded {len(routing_train)} training routing samples\")\n",
    "else:\n",
    "    print(f\"Routing train data not found at {routing_train_path}\")\n",
    "\n",
    "if os.path.exists(routing_test_path):\n",
    "    routing_test = load_jsonl(routing_test_path)\n",
    "    print(f\"Loaded {len(routing_test)} test routing samples\")\n",
    "else:\n",
    "    print(f\"Routing test data not found at {routing_test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Verification\n",
    "\n",
    "Verify that all required data files are available for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all data files\n",
    "required_files = [\n",
    "    ('Query Train Data', CONFIG['output_paths']['query_data_train']),\n",
    "    ('Query Test Data', CONFIG['output_paths']['query_data_test']),\n",
    "    ('Query Embeddings', CONFIG['output_paths']['query_embedding_data']),\n",
    "    ('Routing Train Data', CONFIG['output_paths']['routing_data_train']),\n",
    "    ('Routing Test Data', CONFIG['output_paths']['routing_data_test']),\n",
    "    ('LLM Data', CONFIG['output_paths']['llm_data']),\n",
    "]\n",
    "\n",
    "print(\"Data file verification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_available = True\n",
    "for name, path in required_files:\n",
    "    full_path = str(PROJECT_ROOT / path)\n",
    "    exists = os.path.exists(full_path)\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    if not exists:\n",
    "        all_available = False\n",
    "    print(f\"{name:25} [{status}]\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "if all_available:\n",
    "    print(\"All data files are ready for training!\")\n",
    "else:\n",
    "    print(\"Some files are missing. Please generate them or use example data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "After running this notebook, you should have:\n",
    "\n",
    "1. **Query Data Files**:\n",
    "   - `query_data_train.jsonl` - Training queries\n",
    "   - `query_data_test.jsonl` - Test queries\n",
    "\n",
    "2. **Embedding File**:\n",
    "   - `query_embeddings_longformer.pt` - Unified query embeddings\n",
    "\n",
    "3. **Routing Data Files** (if API calling was performed):\n",
    "   - `routing_data_train.jsonl` - Training routing data with responses\n",
    "   - `routing_data_test.jsonl` - Test routing data with responses\n",
    "\n",
    "4. **LLM Configuration**:\n",
    "   - `default_llm.json` - LLM candidates configuration\n",
    "\n",
    "Now you can proceed to train any of the routers using the method-specific notebooks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
